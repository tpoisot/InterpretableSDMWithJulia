{
  "hash": "78e00a3faea7a9c4206d8fd9f7d88ad3",
  "result": {
    "markdown": "---\ntitle: \"Building an interpretable SDM from sratch\"\nsubtitle: \"With Julia 1.9\"\nauthor:\n    name: \"TimothÃ©e Poisot\"\n    email: timothee.poisot@umontreal.ca\ninstitute: \"UniversitÃ© de MontrÃ©al\"\ntitle-slide-attributes: \n  data-background-image: https://cdn.pixabay.com/photo/2018/07/10/22/09/raccoon-3529806_960_720.jpg\n  data-background-opacity: \"0.15\"\nbibliography: references.bib\ncsl: https://www.zotero.org/styles/ecology-letters\n---\n\n## Overview\n\n-   Build a *simple* classifier to predict the distribution of a species\n\n-   Use this as an opportunity to talk about interpretable ML\n\n-   Discuss which biases are appropriate in a predictive model\n\n------------------------------------------------------------------------\n\n::: r-fit-text\nWe care a lot about the\n\n**process**\n\nand only a little about the\n\n**product**\n:::\n\n------------------------------------------------------------------------\n\n## Raccoons!\n\n-   Relatable (bag under eyes, love naps, out of shape)\n\n-   High volume of data\n\n-   Species of concern for zoonotic diseases\n\n-   Where can we find them in/around QuÃ©bec?\n\nSee also @higino2021 for more quality ðŸ¦ content\n\n## Do try this at home!\n\nðŸ’» + ðŸ“” + ðŸ—ºï¸ at `https://github.com/tpoisot/InterpretableSDMWithJulia/`\n\n::: {#include-the-packages-we-need .cell execution_count=1}\n``` {.julia .cell-code}\ninclude(joinpath(\"code\", \"pkg.jl\")); # Dependencies\ninclude(joinpath(\"code\", \"nbc.jl\")); # Naive Bayes Classifier\ninclude(joinpath(\"code\", \"splitters.jl\")); # Cross-validation\ninclude(joinpath(\"code\", \"confusion.jl\")); # Confusion matrix utilities\ninclude(joinpath(\"code\", \"variableselection.jl\")); # Variable selection\ninclude(joinpath(\"code\", \"shapley.jl\")); # Shapley values\n```\n:::\n\n\n## To train a model, we need...\n\nA response variable $y$\n\n:   presence or absence of a species at a location identified by its latitude and longitude\n\nA series of predictors $\\mathbf{x}$\n\n:   bioclimatic variables\n\nA series of predictions $\\hat y$\n\n:   which we will compare to the values of $y$\n\n## Bioclimatic data\n\nWe collect BioClim data from CHELSA v1, using `SpeciesDistributionToolkit`\n\n::: {#download-the-bioclim-data-from-worldclim2 .cell execution_count=2}\n``` {.julia .cell-code}\nprovider = RasterData(CHELSA1, BioClim)\nopts = (;)\nboundingbox = (bottom=41.0, right=-58.501, left=-80.00, top=51.999)\nsimplifier = (x) -> coarsen(x, mean, (2, 2))\ntemperature = simplifier(SimpleSDMPredictor(provider, layer=1; opts..., boundingbox...))\n```\n:::\n\n\n::: footer\nBioClim data from @karger2020\n:::\n\n## Bioclimatic data\n\nWe set the pixels with only open water to `nothing`\n\n::: {#get-the-open-water-pixels .cell execution_count=3}\n``` {.julia .cell-code}\nwater = simplifier(\n    SimpleSDMPredictor(RasterData(EarthEnv, LandCover), layer=12; opts..., boundingbox...)\n) .< 100\ntemperature = mask(water, temperature)\n```\n:::\n\n\n::: footer\nLand-cover data from @tuanmu2014\n:::\n\n## Species occurrence filtering\n\nWe use the [GBIF] API through the `GBIF` package to get data about *Procyon lotor*\n\n  [GBIF]: http://gbif.org\n\n::: {#get-the-species-from-gbif .cell execution_count=4}\n``` {.julia .cell-code}\ncritter = taxon(\"Procyon lotor\"; strict=false)\n```\n:::\n\n\nWe only consider occurrences within the bounding box!\n\n::: {#get-the-initial-round-of-occurrences .cell execution_count=5}\n``` {.julia .cell-code}\nquery = [\n    \"occurrenceStatus\" => \"PRESENT\",\n    \"hasCoordinate\" => true,\n    \"decimalLatitude\" => (boundingbox.bottom, boundingbox.top),\n    \"decimalLongitude\" => (boundingbox.left, boundingbox.right),\n    \"limit\" => 300,\n]\nobservations = occurrences(critter, query...)\n```\n:::\n\n\n\n\n::: footer\nSee @dansereau2021 for more about these packages\n:::\n\n## Where are we so far?\n\n::: {#015bc3b0 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=36}\n![](slides_files/figure-revealjs/cell-8-output-1.png){}\n:::\n:::\n\n\n## WAIT!\n\nIt's not serious ecology unless we use Phylopic:\n\n::: {#phylopic-image .cell execution_count=8}\n``` {.julia .cell-code}\nphylopic_uuid = Phylopic.imagesof(critter; items = 1)\nsilhouette = phylopic_uuid |>\n    Phylopic.thumbnail |>\n    Downloads.download |>\n    Images.load\n```\n:::\n\n\nðŸ“¢ *Always* use the `Phylopic.attribution` function!\n\n::: footer\nImage of *Procyon lotor* provided by [Margot Michaud](https://creativecommons.org/publicdomain/zero/1.0/)\n\n:::\n\n## Where are we so far?\n\n::: {#bf512896 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display execution_count=39}\n![](slides_files/figure-revealjs/cell-10-output-1.png){}\n:::\n:::\n\n\n## Spatial thinning\n\nWe limit the occurrences to one per grid cell, assigned to the center of the grid cell\n\n::: {#make-the-layer-for-presences .cell execution_count=10}\n``` {.julia .cell-code}\npresence_layer = mask(temperature, observations, Bool)\n```\n:::\n\n\n## Background points generation\n\nWe generate background points in a 200km radius around each point -- but we keep a 20km buffer with no background points:\n\n::: {#make-the-pseudo-absence-buffer .cell execution_count=11}\n``` {.julia .cell-code}\nbackground = pseudoabsencemask(WithinRadius, presence_layer; distance = 200.0)\nbuffer = pseudoabsencemask(WithinRadius, presence_layer; distance = 20.0)\npossible_background = .!(background .| (.! buffer))\n```\n:::\n\n\nAnd then we sample 4 background points out of every 10 occurrences:\n\n::: {#make-the-absence-layer .cell execution_count=12}\n``` {.julia .cell-code}\nabsence_layer = SpeciesDistributionToolkit.sample(\n    possible_background, \n    round(Int, 0.4*sum(presence_layer)),\n    replace=false\n)\n```\n:::\n\n\n::: footer\nSee @barbet-massin2012 for more on background points\n:::\n\n## Background points cleaning\n\nWe can remove all of the information that is neither a presence nor a pseudo-absence\n\n::: {#pseudo-absencepresence-remove .cell execution_count=13}\n``` {.julia .cell-code}\nreplace!(absence_layer, false => nothing)\nreplace!(presence_layer, false => nothing)\n```\n:::\n\n\n## Data overview\n\n::: {#d09238ba .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=44}\n![](slides_files/figure-revealjs/cell-15-output-1.png){}\n:::\n:::\n\n\n\n\n## Preparing the responses and variables\n\n::: {#assemble-y-and-x .cell execution_count=16}\n``` {.julia .cell-code}\nXpresence = hcat([bioclim_var[keys(presence_layer)] for bioclim_var in bioclim_clipped]...)\nypresence = fill(true, length(presence_layer))\nXabsence = hcat([bioclim_var[keys(absence_layer)] for bioclim_var in bioclim_clipped]...)\nyabsence = fill(false, length(absence_layer))\nX = vcat(Xpresence, Xabsence)\ny = vcat(ypresence, yabsence)\n```\n:::\n\n\n\n\n## The model - Naive Bayes Classifier\n\nPrediction:\n\n$$\nP(+|x) = \\frac{P(+)}{P(x)}P(x|+)\n$$\n\nDecision rule:\n\n$$\n\\hat y = \\text{argmax}_j \\, P(\\mathbf{c}_j)\\prod_i P(\\mathbf{x}_i|\\mathbf{c}_j)\n$$\n\n::: footer\nWith $n$ instances and $f$ features, NBC trains *and* predicts in $\\mathcal{O}(n\\times f)$\n:::\n\n## The model -- Naive Bayes Classifier\n\nAssumption of Gaussian distributions:\n\n$$\nP(x|+) = \\text{pdf}(x, \\mathcal{N}(\\mu_+, \\sigma_+))\n$$\n\n## Cross-validation\n\nWe keep an **unseen** *testing* set -- this will be used at the very end to report expected model performance\n\n::: {#testing-set .cell execution_count=18}\n``` {.julia .cell-code}\nidx, tidx = holdout(y, X; permute=true)\n```\n:::\n\n\nFor *validation*, we will run k-folds\n\n::: {#k-folds .cell execution_count=19}\n``` {.julia .cell-code}\nty, tX = y[idx], X[idx,:]\nfolds = kfold(ty, tX; k=15, permute=true)\nk = length(folds)\n```\n:::\n\n\n::: footer\nSee @valavi2018 for more on cross-validation\n:::\n\n## A note on cross-validation\n\nAll models share the same folds\n\n:   we can compare the validation performance across experiments to select the best model\n\nModel performance can be compared\n\n:   we average the relevant summary statistics over each validation set\n\nTesting set is *only* for future evaluation\n\n:   we can only use it once and report the expected performance *of the best model*\n\n## Baseline performance\n\nWe need to get a sense of how difficult the classification problem is:\n\n::: {#e5da6d12 .cell execution_count=20}\n``` {.julia .cell-code}\nC0 = zeros(ConfusionMatrix, length(folds))\nfor (i,f) in enumerate(folds)\n    trn, val = f\n    foldmodel = naivebayes(ty[trn], tX[trn,:])\n    foldpred = vec(mapslices(foldmodel, tX[val,:]; dims=2))\n    C0[i] = ConfusionMatrix(foldpred, ty[val])\nend\n```\n:::\n\n\nThis uses an un-tuned model with all variables and reports the average over all validation sets\n\n## Measures on the confusion matrix\n\n|     | Initial                                   |\n|-----|-------------------------------------------|\n| FPR | 0.16 |\n| FNR | 0.06 |\n| TPR | 0.94 |\n| TNR | 0.84 |\n| MCC | 0.78 |\n\n::: footer\nIt's a good idea to check the values for the training sets too...\n:::\n\n## Variable selection\n\nWe add variables one at a time, until the Matthew's Correlation Coefficient stops increasing -- we keep annual temperature, isothermality, mean diurnal range, and annual precipitation\n\n::: {#b03ee5d3 .cell execution_count=21}\n``` {.julia .cell-code}\navailable_variables = constrainedselection(ty, tX, folds, naivebayes, mcc, [1, 2, 3, 12])\n```\n:::\n\n\nThis method identifies 7 variables, some of which are:\n\n1.  Annual Mean Temperature\n\n2.  Mean Diurnal Range \\(Mean of monthly \\(max temp \\- min temp\\)\\)\n\n3.  Isothermality \\(BIO2/BIO7\\) \\(Ã—100\\)\n\n## Discuss - can we force variable selection?\n\n-   constrained variable selection\n\n-   VIF + variable selection\n\n-   PCA?\n\n## Model with variable selection\n\n::: {#f4b10618 .cell execution_count=22}\n``` {.julia .cell-code}\nC1 = zeros(ConfusionMatrix, length(folds))\nfor (i,f) in enumerate(folds)\n    trn, val = f\n    foldmodel = naivebayes(ty[trn], tX[trn,available_variables])\n    foldpred = vec(mapslices(foldmodel, tX[val,available_variables]; dims=2))\n    C1[i] = ConfusionMatrix(foldpred, ty[val])\nend\n```\n:::\n\n\n## Measures on the confusion matrix\n\n|     | Initial                                   | Var. sel.                                 |\n|---------------------|--------------------------|--------------------------|\n| FPR | 0.16 | 0.15 |\n| FNR | 0.06 | 0.05 |\n| TPR | 0.94 | 0.95 |\n| TNR | 0.84 | 0.85 |\n| MCC | 0.78 | 0.8 |\n\n## How do we make the model better?\n\nThe NBC is a *probabilistic classifier* returning $P(+|\\mathbf{x})$\n\nThe *decision rule* is to assign a presence when $P(\\cdot) > 0.5$\n\nBut $P(\\cdot) > \\tau$ is a far more general approach, and we can use learning curves to identify $\\tau$\n\n## Thresholding the model\n\n::: {#7ce552e7 .cell execution_count=23}\n``` {.julia .cell-code}\nty, tX = y[idx], X[idx,available_variables]\nthr = LinRange(0.0, 1.0, 350)\nC = zeros(ConfusionMatrix, (k, length(thr)))\nfor (j,fold) in enumerate(folds)\n    trn, vld = fold\n    foldmodel = naivebayes(ty[trn], tX[trn,:])\n    foldvalid = vec(mapslices(foldmodel, tX[vld,:]; dims=2))\n    for (i,t) in enumerate(thr)\n        C[j,i] = ConfusionMatrix(foldvalid, ty[vld], t)\n    end\nend\n```\n:::\n\n\n## But how do we pick the threshold?\n\n::: {#c5d5d989 .cell execution_count=24}\n\n::: {.cell-output .cell-output-display execution_count=57}\n![](slides_files/figure-revealjs/cell-25-output-1.svg){}\n:::\n:::\n\n\n## Tuned model with selected variables\n\n::: {#0580db00 .cell execution_count=25}\n``` {.julia .cell-code}\nC2 = zeros(ConfusionMatrix, length(folds))\nfor (i,f) in enumerate(folds)\n    trn, val = f\n    foldmodel = naivebayes(ty[trn], tX[trn,:])\n    foldpred = vec(mapslices(foldmodel, tX[val,:]; dims=2))\n    C2[i] = ConfusionMatrix(foldpred, ty[val], thr[m])\nend\n```\n:::\n\n\n## Measures on the confusion matrix\n\n|     | Initial                                   | Var. sel.                                 | Tuned                                     |\n|------------------|------------------|------------------|------------------|\n| FPR | 0.16 | 0.15 | 0.19 |\n| FNR | 0.06 | 0.05 | 0.03 |\n| TPR | 0.94 | 0.95 | 0.97 |\n| TNR | 0.84 | 0.85 | 0.81 |\n| MCC | 0.78 | 0.8 | 0.82 |\n\n## Tuned model performance\n\nWe can retrain over *all* the training data\n\n::: {#a61083f9 .cell execution_count=26}\n``` {.julia .cell-code}\nfinalmodel = naivebayes(ty, tX)\nprediction = vec(mapslices(finalmodel, X[tidx,available_variables]; dims=2))\nCf = ConfusionMatrix(prediction, y[tidx], thr[m])\n```\n:::\n\n\n## Estimated performance\n\n|     | Final model                        |\n|-----|------------------------------------|\n| FPR | 0.18 |\n| FNR | 0.02 |\n| TPR | 0.98 |\n| TNR | 0.82 |\n| MCC | 0.83 |\n\n## Acceptable bias\n\n-   false positives: we expect that our knowledge of the distribution is incomplete!\n\n-   false negatives: we used a heuristic for background points!\n\n## Prediction for each pixel\n\n\n\n::: {#9f18a9d9 .cell execution_count=28}\n``` {.julia .cell-code}\nprediction = similar(first(predictors))\nThreads.@threads for k in keys(prediction)\n    prediction[k] = finalmodel([p[k] for p in predictors[available_variables]])\n    if isnan(prediction[k])\n        prediction[k] = 0.0\n    end\nend\n```\n:::\n\n\n## Tuned model - prediction\n\n::: {#000ac1a3 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display execution_count=64}\n![](slides_files/figure-revealjs/cell-30-output-1.png){}\n:::\n:::\n\n\n## Tuned model - uncertainty\n\n::: {#ec6496b6 .cell execution_count=30}\n\n::: {.cell-output .cell-output-display execution_count=65}\n![](slides_files/figure-revealjs/cell-31-output-1.png){}\n:::\n:::\n\n\n::: footer\nIQR for the models trained on each fold\n:::\n\n## Tuned model - entropy\n\n::: {#49a35df7 .cell execution_count=31}\n\n::: {.cell-output .cell-output-display execution_count=66}\n![](slides_files/figure-revealjs/cell-32-output-1.png){}\n:::\n:::\n\n\n::: footer\nEntropy (in bits) of the NBC probability\n:::\n\n## Tuned model - range\n\n::: {#0375aeac .cell execution_count=32}\n\n::: {.cell-output .cell-output-display execution_count=67}\n![](slides_files/figure-revealjs/cell-33-output-1.png){}\n:::\n:::\n\n\n::: footer\nProbability \\> 0.146\n:::\n\n## Predicting the predictions?\n\nShapley values (Monte-Carlo approximation): if we mix the variables across two observations, how important is the $i$-th variable?\n\nExpresses \"importance\" as an additive factor on top of the *average* prediction (here: average prob. of occurrence)\n\n::: {#22257ce8 .cell execution_count=33}\n``` {.julia .cell-code}\nshapval = [similar(first(predictors)) for i in eachindex(available_variables)]\nThreads.@threads for k in keys(shapval[1])\n    x = [p[k] for p in predictors[available_variables]]\n    for i in axes(shapval, 1)\n        shapval[i][k] = shapleyvalues(finalmodel, tX, x, i; M=50)\n        if isnan(shapval[i][k])\n            shapval[i][k] = 0.0\n        end\n    end\nend\n```\n:::\n\n\n## Importance of variables\n\n::: {#3ba43481 .cell execution_count=34}\n``` {.julia .cell-code}\nvarimp = sum.(map(abs, shapval))\nvarimp ./= sum(varimp)\nfor v in sortperm(varimp, rev=true)\n    vname = variables[available_variables[v]][2]\n    vctr = round(Int, varimp[v]*100)\n    println(\"$(vname) - $(vctr)%\")\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnnual Mean Temperature - 44%\nMean Temperature of Warmest Quarter - 42%\nIsothermality (BIO2/BIO7) (Ã—100) - 4%\nMean Diurnal Range (Mean of monthly (max temp - min temp)) - 4%\nPrecipitation of Driest Month - 4%\nPrecipitation of Coldest Quarter - 3%\nAnnual Precipitation - 0%\n```\n:::\n:::\n\n\nThere is a difference between **contributing to model performance** and **contributing to model explainability**\n\n## Top three variables\n\n::: {#13e26e4b .cell execution_count=35}\n\n::: {.cell-output .cell-output-display execution_count=71}\n![](slides_files/figure-revealjs/cell-36-output-1.png){}\n:::\n:::\n\n\n## Most determinant predictor\n\n::: {#ee343e3f .cell execution_count=36}\n\n::: {.cell-output .cell-output-display execution_count=72}\n![](slides_files/figure-revealjs/cell-37-output-1.png){}\n:::\n:::\n\n\n## Take-home\n\n-   building a model is *incremental*\n\n-   each step adds arbitrary decisions we can control for, justify, or live with\n\n-   we can provide explanations for every single prediction\n\n-   free online textbook (in development) at `https://tpoisot.github.io/DataSciForBiodivSci/`\n\n## References\n\n",
    "supporting": [
      "slides_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}